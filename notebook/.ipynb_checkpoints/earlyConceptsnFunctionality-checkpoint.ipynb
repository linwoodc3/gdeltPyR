{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GDELT 1.0 Code (skip)\n",
    "\n",
    "Will have to integrate this into GDELT 2.0.  Headers are different.  GDELT 1.0 goes back to 1979.  2.0 only goes back to Feb 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(url='../utils/images/spinningglobe.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import lxml.html as lh\n",
    "\n",
    "gdelt_base_url = 'http://data.gdeltproject.org/events/'\n",
    "gdelt_gkg_url = 'http://api.gdeltproject.org/api/v1/gkg_geojson'\n",
    "# get the list of all the links on the gdelt file page\n",
    "page = requests.get(gdelt_base_url+'index.html')\n",
    "doc = lh.fromstring(page.content)\n",
    "link_list = doc.xpath(\"//*/ul/li/a/@href\")\n",
    "\n",
    "# separate out those links that begin with four digits \n",
    "file_list = [x for x in link_list if str.isdigit(x[0:4])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.read_excel('./CSV.header.fieldids.xlsx').columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = downloadAndExtract(gdelt_base_url+file_list[0])\n",
    "df.columns = pd.read_excel('./CSV.header.fieldids.xlsx').columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.ix[6566]['SOURCEURL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.ix[629]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.EventCode[\n",
    "    (df.GoldsteinScale <-5) &\n",
    "    (df.ActionGeo_Lat.notnull() ) &\n",
    "    (df.NumArticles>1000) &\n",
    "    (df.ActionGeo_CountryCode=='SY')\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "masterListUrl = 'http://data.gdeltproject.org/gdeltv2/masterfilelist.txt'\n",
    "directory = requests.get(masterListUrl)\n",
    "results = directory.content.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.options.display.max_rows = 200\n",
    "# df = pd.DataFrame(data.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['coords'] = df.features.apply(lambda row: row['geometry']['coordinates'])\n",
    "df['lat'] = df.features.apply(lambda row: row['geometry']['coordinates'][1])\n",
    "df['lon'] = df.features.apply(lambda row: row['geometry']['coordinates'][0])\n",
    "df['name'] = df.features.apply(lambda row: row['properties']['name'])\n",
    "df['pubdate'] = df.features.apply(lambda row: row['properties']['urlpubtimedate'])\n",
    "df['urltone'] = df.features.apply(lambda row: row['properties']['urltone'])\n",
    "df['mentionedNames'] = df.features.apply(lambda row: row['properties']['mentionednames'])\n",
    "df['mentioinedThemes'] = df.features.apply(lambda row: row['properties']['mentionedthemes'])\n",
    "df['url'] = df.features.apply(lambda row: row['properties']['url'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GDELT 2.0 Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from dateutil.parser import parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logic for GDELT module\n",
    "\n",
    "Enter a date or date range.  GDELT 2.0 only goes to Feb 18 2015.  GDELT 1.0 goes back to 1979.  \n",
    "\n",
    "Convert the entered date or date range to string, search for string in the master df list.  Use the tblType parameter to pull the correct table(s).  \n",
    "\n",
    "* default is take current time and most recent file\n",
    "* enter historical date; defaults to last record of day\n",
    "    * parse\n",
    "    * add feature to enter time for historical and pull closest 15 minute file\n",
    "    * date range will pull last file for each day and concatenate into single dataframe\n",
    "    \n",
    "choose a database\n",
    "*  Select between events, event mentions or gkg\n",
    "\n",
    "return it as a python or R dataframe\n",
    "*  use the feather library for Python\n",
    "\n",
    "*********************\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dateutil.parser import parse\n",
    "import traceback,sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import requests\n",
    "\n",
    "\n",
    "class gdeltSearch(object):\n",
    "    \"\"\"Placeholder string\"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 gdelt2MasterUrl = 'http://data.gdeltproject.org/gdeltv2/masterfilelist.txt',\n",
    "                 gdelt1MasterUrl = 'http://data.gdeltproject.org/events/index.html',\n",
    "                 tblType = None,\n",
    "                 headers = None,\n",
    "                 masterdf = None,\n",
    "                 clean = None,\n",
    "                 queryTime = datetime.datetime.now().strftime('%m-%d-%Y %H:%M:%S')\n",
    "                 ):\n",
    "        self.gdelt2MasterUrl=gdelt2MasterUrl\n",
    "        self.gdelt1MasterUrl=gdelt1MasterUrl\n",
    "        self.clean = map(\n",
    "                    lambda x: x.split(' '),\n",
    "                    requests.get(self.gdelt2MasterUrl).content.split('\\n')\n",
    "                        )\n",
    "        del self.clean[-1]\n",
    "        self.masterdf = pd.DataFrame(self.clean)\n",
    "        self.masterdf.fillna('', inplace=True)\n",
    "        self.queryTime=queryTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "masterdf = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import traceback,sys\n",
    "import datetime\n",
    "from dateutil.parser import parse\n",
    "import numpy as np\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class gdeltSearch(object):\n",
    "    \"\"\"Placeholder string\"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 gdelt2MasterUrl='http://data.gdeltproject.org/gdeltv2/masterfilelist.txt',\n",
    "                 gdelt1MasterUrl='http://data.gdeltproject.org/events/index.html',\n",
    "                 tblType=None,\n",
    "                 headers=None,\n",
    "                 masterdf=None,\n",
    "                 clean = None,\n",
    "                 ):\n",
    "        self.gdelt2MasterUrl=gdelt2MasterUrl\n",
    "        self.gdelt1MasterUrl=gdelt1MasterUrl\n",
    "        self.clean = map(lambda x: x.split(' '),requests.get(self.gdelt2MasterUrl).content.split('\\n'))\n",
    "        del self.clean[-1]\n",
    "        self.masterdf = pd.DataFrame(self.clean)\n",
    "        self.masterdf.fillna('',inplace=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    def dateInputCheck(date):\n",
    "        \"\"\"Function to check date entered by user.\n",
    "\n",
    "        Example\n",
    "\n",
    "        Parameters\n",
    "            ----------\n",
    "            date : {string or list}, \n",
    "                Input data, where ``date`` is a single date string, \n",
    "                two dates representing a range, or several dates \\\n",
    "                that represent individual days of interest.\n",
    "        Returns\n",
    "        -------\n",
    "        self : None\n",
    "            Returns self.\n",
    "        \"\"\"\n",
    "\n",
    "        if isinstance(date,str):\n",
    "            if date != \"\":\n",
    "                if parse(date) > datetime.datetime.now():\n",
    "                    raise ValueError('Your date is greater than the current date.\\\n",
    "                    Please enter a relevant date.')\n",
    "                elif parse(date)<parse('Feb 18 2015'):\n",
    "                    raise ValueError('GDELT 2.0 only supports \\'Feb 18 2015 - Present\\'\\\n",
    "                    queries currently. Try another date.')\n",
    "\n",
    "        elif isinstance(date,list):\n",
    "            if len(date)==1:\n",
    "                try:\n",
    "                    if parse(\"\".join(date)) > datetime.datetime.now():\n",
    "                        raise ValueError('Your date is greater than the current\\\n",
    "                        date.  Please enter a relevant date.')\n",
    "                    elif parse(\"\".join(date)) < parse('Feb 18 2015'):\n",
    "                        raise ValueError('GDELT 2.0 only supports \\'Feb 18 2015 - Present\\' \\\n",
    "                        queries currently. Try another date.')\n",
    "                except:\n",
    "                    exc_type, exc_value, exc_traceback = sys.exc_info()\n",
    "                    traceback.print_tb(exc_traceback, limit=1, file=sys.stdout)\n",
    "                    traceback.print_exception(exc_type, exc_value, exc_traceback,\n",
    "                                              limit=2, file=sys.stdout)\n",
    "                    raise ValueError(\"One or more of your input date strings does \\\n",
    "                    not parse to a date format. Check input.\")\n",
    "\n",
    "\n",
    "            elif len(date)==2:\n",
    "                try:\n",
    "                    map(parse,date)\n",
    "                except Exception as exc:\n",
    "                    exc_type, exc_value, exc_traceback = sys.exc_info()\n",
    "                    traceback.print_tb(exc_traceback, limit=1, file=sys.stdout)\n",
    "                    traceback.print_exception(exc_type, exc_value, exc_traceback,\n",
    "                                              limit=2, file=sys.stdout)\n",
    "                    raise ValueError(\"One or more of your input date strings \\\n",
    "                    does not parse to a date format. Check input.\")\n",
    "\n",
    "                if bool(parse(date[0])<parse(date[1])) == False:\n",
    "                    raise ValueError('Start date greater than end date. Check date \\\n",
    "                    strings.')\n",
    "\n",
    "                if np.all(\n",
    "                        np.logical_not(np.array(map(parse,date))> datetime.datetime.now())\n",
    "                        ) == False:\n",
    "                    raise ValueError(\"One of your dates is greater than the current \\\n",
    "                    date. Check input date strings.\")\n",
    "\n",
    "\n",
    "            elif len(date)>2:\n",
    "\n",
    "                try:\n",
    "                    map(parse,date)\n",
    "                except Exception as exc:\n",
    "                    exc_type, exc_value, exc_traceback = sys.exc_info()\n",
    "                    traceback.print_tb(exc_traceback, limit=1, file=sys.stdout)\n",
    "                    traceback.print_exception(exc_type, exc_value, exc_traceback,\n",
    "                                              limit=2, file=sys.stdout)\n",
    "                    raise ValueError(\"One or more of your input date strings does \\\n",
    "                    not parse to a date format. Check input.\")\n",
    "\n",
    "                if np.all(\n",
    "                        np.logical_not(np.array(map(parse,date))> datetime.datetime.now())\n",
    "                        ) == False:\n",
    "                    raise ValueError(\"One or more of your input date strings does not \\\n",
    "                    parse to a date format. Check input.\")\n",
    "                    \n",
    "            self.date=date\n",
    "            \n",
    "    @staticmethod        \n",
    "    def parse_date(var):\n",
    "        \"\"\"Return datetime object from string.\"\"\"\n",
    "\n",
    "        try:\n",
    "            return np.where(isinstance(parse(var),datetime.datetime),\n",
    "                     parse(var),\"Error\")             \n",
    "        except:\n",
    "            return \"You entered an incorrect date.  Check your date format.\"\n",
    "\n",
    "    @staticmethod\n",
    "    def dateformatter(datearray):\n",
    "        \"\"\"Function to format strings for numpy arange\"\"\"\n",
    "        return parse(datearray).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    @staticmethod\n",
    "    def dateRanger(originalArray):\n",
    "        \"\"\"Function to vectorize date formatting function.\n",
    "        Creates datetime.date objects for each day in the range\n",
    "        and stores in a numpy array.\n",
    "\n",
    "        Example\n",
    "\n",
    "        Parameters\n",
    "            ----------\n",
    "            originalArray : {array-like}, List of date strings \\\n",
    "            to query.\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns array.\n",
    "        \"\"\"\n",
    "        if isinstance(originalArray,str):\n",
    "            \"\"\"Check user input to retrieve date query.\"\"\"\n",
    "\n",
    "            return np.where(len(originalArray)==0,np.array(datetime.datetime.now()),\n",
    "                     parse_date(originalArray))\n",
    "\n",
    "        elif isinstance(originalArray,list):\n",
    "            if len(originalArray)==1:\n",
    "                return np.array(parse(\"\".join(originalArray)))\n",
    "            elif len(originalArray)>2:\n",
    "                return np.array(map(parse,originalArray),dtype='datetime64[D]')\n",
    "            else:\n",
    "                cleaner = np.vectorize(dateformatter)\n",
    "                converted = cleaner(originalArray).tolist()\n",
    "                dates = np.arange(converted[0],converted[1],dtype='datetime64[D]')\n",
    "                dates = np.append(dates,np.datetime64(datetime.date.today())) \n",
    "                return dates\n",
    "    @staticmethod\n",
    "    def gdeltRangeString(element):\n",
    "        if element == datetime.date.today():\n",
    "            multiplier = datetime.datetime.now().minute / 15\n",
    "            multiple = 15 * multiplier\n",
    "            converted = datetime.datetime.now().replace(minute=multiple,second=0)\n",
    "        else:\n",
    "            converted = (datetime.datetime.combine(element,datetime.time.min) + \n",
    "                datetime.timedelta(\n",
    "                                    minutes=45,hours=23\n",
    "                                    )\n",
    "                                   )\n",
    "        return converted.strftime('%Y%m%d%H%M%S')\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def vectorizer(function,dateArray):\n",
    "        helper = np.vectorize(function)\n",
    "        return helper(dateArray.tolist()).tolist()\n",
    "\n",
    "    # Finds the urls from an array of dates\n",
    "\n",
    "    @staticmethod\n",
    "    def UrlFinder(targetDate):\n",
    "        return masterdf[masterdf[2].str.contains(targetDate)]\n",
    "\n",
    "    @staticmethod\n",
    "    def vectorizedUrlFinder(function,urlList):\n",
    "        helper=np.vectorize(function)\n",
    "        return pd.concat(helper(urlList).tolist())\n",
    "\n",
    "    @staticmethod\n",
    "    def downloadVectorizer(function,urlList):\n",
    "        '''\n",
    "        test2 = downloadVectorizer(downloadAndExtract,b)\n",
    "        test2.columns=gkgHeaders.tableId.tolist()\n",
    "        '''\n",
    "        helper=np.vectorize(function)\n",
    "        return pd.concat(helper(urlList).tolist())\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gdelt = gdeltSearch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gdelt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# URLS\n",
    "\n",
    "The main urls that we need to hit to return data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "masterListUrl = 'http://data.gdeltproject.org/gdeltv2/masterfilelist.txt'\n",
    "baseUrl = 'http://data.gdeltproject.org/gdeltv2/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters and Global Variables\n",
    "\n",
    "Section contains variables that will be `self.` objects in the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Listing of all GDELT 15 minute dumps. Code retrieves the list,\n",
    "splits it on the new line character, and then splits on the space. \n",
    "We delete the last entry because it's empty.  \n",
    "'''\n",
    "directory = requests.get(masterListUrl)\n",
    "clean = directory.content.split('\\n')\n",
    "clean = map(lambda x: x.split(' '),clean)\n",
    "del clean[-1]\n",
    "\n",
    "\"\"\"\n",
    "Setting up the master list as dataframe for querying\n",
    "this will be inside the class\n",
    "\"\"\"\n",
    "masterdf = pd.DataFrame(clean)\n",
    "masterdf.fillna('',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dayFull = downloadVectorizer(downloadAndExtract,masterdf[(masterdf[2].str.contains('/20160926')) & (masterdf[2].str.contains('export'))][2].unique().tolist())\n",
    "dayFull.columns = eventsDbHeaders.tableId.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dayFullMentions = downloadVectorizer(downloadAndExtract,masterdf[(masterdf[2].str.contains('/20160926')) & (masterdf[2].str.contains('mentions'))][2].unique().tolist())\n",
    "dayFullMentions.columns = mentionsHeaders.tableId.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "finalFull = dayFull.merge(dayFullMentions,how='outer',on='GLOBALEVENTID')\n",
    "finalFull.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "finalFull.EventCode>=190"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "finalFull[\n",
    "    (finalFull.Confidence > 75) & \n",
    "    (finalFull.ActionGeo_Lat.notnull()) & \n",
    "    (finalFull.GoldsteinScale<-5) &\n",
    "    (finalFull.ActionGeo_CountryCode=='SY') &\n",
    "    (finalFull.EventCode==193)\n",
    "        ].SOURCEURL.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "finalFull.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "finalFull[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "finalFull[(finalFull.ActionGeo_CountryCode=='SY') & \n",
    "          (finalFull.Actor1Name.notnull()) &\n",
    "          (finalFull.Confidence >70) &\n",
    "          (finalFull.EventCode>190) &\n",
    "          (finalFull.Actor1Name.str.contains('TERRORIST'))].SOURCEURL.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# table type = tblType\n",
    "graph = 'gkg'\n",
    "events = 'events' # includes new GDELT 2.0 mentions table; merged on globaleventid\n",
    "\n",
    "tblType = events  # default to events db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Date Parameters that will be entered\n",
    "\n",
    "Location to hold testing spot for all the different type of parameters that can be entered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "defaultDateEntry = \"\" # string\n",
    "stringDateEntry = \" 2016 09 18\" # string\n",
    "historicalDateEntry = \"2015 02 25\" #string\n",
    "errorDate = \"What in the heck\" # error string\n",
    "listOfdates = ['Sep 1 2016','2016 09 24'] # list, len 2\n",
    "moreThanTwo= ['Sept 20 2016','June 3 2011','January 1, 2013'] # list, len greater than 2d\n",
    "\n",
    "date = defaultDateEntry\n",
    "time = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the values for the headers\n",
    "\n",
    "Headers are set based on `tblType` value passed in.  Will default to the events DB headers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gkgHeaders = pd.read_csv(\n",
    "    '../utils/schema_csvs/GDELT_2.0_gdeltKnowledgeGraph_Column_Labels_Header_Row_Sep2016.tsv',\n",
    "    delimiter='\\t',usecols=['tableId','dataType','Description']\n",
    "    )\n",
    "gkgHeaders.tableId.tolist();\n",
    "\n",
    "eventsDbHeaders = pd.read_csv('../utils/schema_csvs/GDELT_2.0_Events_Column_Labels_Header_Row_Sep2016.csv',\n",
    "                         delimiter=',',usecols=['tableId','dataType','Description'])\n",
    "eventsDbHeaders.tableId.tolist();\n",
    "\n",
    "mentionsHeaders = pd.read_csv('../utils/schema_csvs/GDELT_2.0_eventMentions_Column_Labels_Header_Row_Sep2016.tsv',\n",
    "                         delimiter='\\t',usecols=['tableId','dataType','Description'])\n",
    "mentionsHeaders.tableId.tolist();\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datetime.datetime.now().strftime('%m-%d-%Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**************************\n",
    "\n",
    "\n",
    "# Checking Inputs of functions and parameters\n",
    "\n",
    "We need to see how many dates are passed into the function.  Use the logic above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import traceback,sys\n",
    "import datetime\n",
    "from dateutil.parser import parse\n",
    "import numpy as np\n",
    "\n",
    "def dateInputCheck(date):\n",
    "    \"\"\"Function to check date entered by user.\n",
    "    \n",
    "    Example\n",
    "    \n",
    "    Parameters\n",
    "        ----------\n",
    "        date : {string or list}, \n",
    "            Input data, where ``date`` is a single date string, \n",
    "            two dates representing a range, or several dates \\\n",
    "            that represent individual days of interest.\n",
    "    Returns\n",
    "    -------\n",
    "    self : None\n",
    "        Returns self.\n",
    "    \"\"\"\n",
    "    \n",
    "    if isinstance(date,str):\n",
    "        if date != \"\":\n",
    "            if parse(date) > datetime.datetime.now():\n",
    "                raise ValueError('Your date is greater than the current date.\\\n",
    "                Please enter a relevant date.')\n",
    "            elif parse(date)<parse('Feb 18 2015'):\n",
    "                raise ValueError('GDELT 2.0 only supports \\'Feb 18 2015 - Present\\'\\\n",
    "                queries currently. Try another date.')\n",
    "\n",
    "    elif isinstance(date,list):\n",
    "        if len(date)==1:\n",
    "            try:\n",
    "                if parse(\"\".join(date)) > datetime.datetime.now():\n",
    "                    raise ValueError('Your date is greater than the current\\\n",
    "                    date.  Please enter a relevant date.')\n",
    "                elif parse(\"\".join(date)) < parse('Feb 18 2015'):\n",
    "                    raise ValueError('GDELT 2.0 only supports \\'Feb 18 2015 - Present\\' \\\n",
    "                    queries currently. Try another date.')\n",
    "            except:\n",
    "                exc_type, exc_value, exc_traceback = sys.exc_info()\n",
    "                traceback.print_tb(exc_traceback, limit=1, file=sys.stdout)\n",
    "                traceback.print_exception(exc_type, exc_value, exc_traceback,\n",
    "                                          limit=2, file=sys.stdout)\n",
    "                raise ValueError(\"One or more of your input date strings does \\\n",
    "                not parse to a date format. Check input.\")\n",
    "\n",
    "        \n",
    "        elif len(date)==2:\n",
    "            try:\n",
    "                map(parse,date)\n",
    "            except Exception as exc:\n",
    "                exc_type, exc_value, exc_traceback = sys.exc_info()\n",
    "                traceback.print_tb(exc_traceback, limit=1, file=sys.stdout)\n",
    "                traceback.print_exception(exc_type, exc_value, exc_traceback,\n",
    "                                          limit=2, file=sys.stdout)\n",
    "                raise ValueError(\"One or more of your input date strings \\\n",
    "                does not parse to a date format. Check input.\")\n",
    "\n",
    "            if bool(parse(date[0])<parse(date[1])) == False:\n",
    "                raise ValueError('Start date greater than end date. Check date \\\n",
    "                strings.')\n",
    "                \n",
    "            if np.all(\n",
    "                    np.logical_not(np.array(map(parse,date))> datetime.datetime.now())\n",
    "                    ) == False:\n",
    "                raise ValueError(\"One of your dates is greater than the current \\\n",
    "                date. Check input date strings.\")\n",
    "\n",
    "            \n",
    "        elif len(date)>2:\n",
    "\n",
    "            try:\n",
    "                map(parse,date)\n",
    "            except Exception as exc:\n",
    "                exc_type, exc_value, exc_traceback = sys.exc_info()\n",
    "                traceback.print_tb(exc_traceback, limit=1, file=sys.stdout)\n",
    "                traceback.print_exception(exc_type, exc_value, exc_traceback,\n",
    "                                          limit=2, file=sys.stdout)\n",
    "                raise ValueError(\"One or more of your input date strings does \\\n",
    "                not parse to a date format. Check input.\")\n",
    "                \n",
    "            if np.all(\n",
    "                    np.logical_not(np.array(map(parse,date))> datetime.datetime.now())\n",
    "                    ) == False:\n",
    "                raise ValueError(\"One or more of your input date strings does not \\\n",
    "                parse to a date format. Check input.\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "date=['2016 9 12']\n",
    "dateInputCheck(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dateInputCheck('October')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the tblType input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# gets the urls from array\n",
    "# resultMaster = vectorizedUrlFinder(UrlFinder,datesToPull)\n",
    "\n",
    "\n",
    "def tblCheck(tbl):\n",
    "    '''Checking the input of tblType.'''\n",
    "    if tbl == 'events' or tbl == '' or tbl == 'mentions':\n",
    "        resultsUrlList = resultMaster[2][resultMaster[2].str.contains('export|mentions')]\n",
    "    elif tbl == 'gkg':\n",
    "        resultsUrlList = resultMaster[2][resultMaster[2].str.contains('gkg')]\n",
    "    else:\n",
    "        raise ValueError (\"Incorrect parameter \\'{0}\\' entered.  Did you mean to use \\'{0}\\' as the parameter?\\nPlease check your \\'tblType\\' parameters.\".format(tblType))\n",
    "    return resultsUrlList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Date Functionality (Date ranges)\n",
    "\n",
    "Use the numpy date range functionality to create strings of dates between ranges in a list.  Then, use the dateutil tool to parse those strings into the correct format.  Then run a query for each date, return the dataframe, and concatenate into a single one.\n",
    "\n",
    "* Logic\n",
    "    * If length of passed in date less than zero, raise error\n",
    "    * If length is equal to one, find that one date's table or graph\n",
    "    * If length equal to two:\n",
    "        * if dates are chronological, covert to numpy range and pull all tables or graphs, but raise warning for long ranges\n",
    "        * if dates are not chronological, get individual dates\n",
    "    * If length greater than two, get the individual dates\n",
    "        * initially, return the latest time\n",
    "        * add option to return closest 15 minute interval to passed in time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Pieces and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import datetime\n",
    "# d = datetime.date().today()\n",
    "# datetime.combine(d, datetime.min.time())\n",
    "\n",
    "import datetime\n",
    "datetime.datetime.combine(parse(datearray),datetime.datetime.min.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# numpy example of ranging the date\n",
    "np.arange('2016-08-01', '2016-09-16', dtype='datetime64[D]')[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Parse the date\n",
    "#############################################\n",
    "\n",
    "\n",
    "from dateutil.parser import parse\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import requests\n",
    "import datetime\n",
    "\n",
    "\n",
    "\n",
    "def parse_date(var):\n",
    "    \"\"\"Return datetime object from string.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        return np.where(isinstance(parse(var),datetime.datetime),\n",
    "                 parse(var),\"Error\")             \n",
    "    except:\n",
    "        return \"You entered an incorrect date.  Check your date format.\"\n",
    "\n",
    "\n",
    "def dateFormatter(datearray):\n",
    "    \"\"\"Function to format strings for numpy arange\"\"\"\n",
    "    return parse(datearray).strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "\n",
    "def dateRanger(originalArray):\n",
    "    \"\"\"Function to vectorize date formatting function.\n",
    "    Creates datetime.date objects for each day in the range\n",
    "    and stores in a numpy array.\n",
    "    \n",
    "    Example\n",
    "    \n",
    "    Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
    "            Input data, where ``n_samples`` is the number of samples and\n",
    "            ``n_features`` is the number of features.\n",
    "    Returns\n",
    "    -------\n",
    "    self : object\n",
    "        Returns self.\n",
    "    \"\"\"\n",
    "    if isinstance(originalArray,str):\n",
    "        \"\"\"Check user input to retrieve date query.\"\"\"\n",
    "    \n",
    "        return np.where(len(originalArray)==0,np.array(datetime.datetime.now()),\n",
    "                 parse_date(originalArray))\n",
    "    \n",
    "    elif isinstance(originalArray,list):\n",
    "       \n",
    "        if len(originalArray)==1:\n",
    "            return np.array(parse(\"\".join(originalArray)))\n",
    "        elif len(originalArray)>2:\n",
    "#           \n",
    "#             return np.array(map(parse,originalArray),dtype='datetime64[D]').tolist()\n",
    "            return np.array(map(lambda x: parse(x),originalArray))\n",
    "        else:\n",
    "            \n",
    "            cleaner = np.vectorize(dateFormatter)\n",
    "            converted = cleaner(originalArray).tolist()\n",
    "            dates = np.arange(converted[0],converted[1],dtype='datetime64[D]')\n",
    "            dates = map(lambda x: datetime.datetime.combine(x,datetime.datetime.min.time()),dates.tolist())\n",
    "            if len(originalArray)==2:\n",
    "                adder = np.datetime64(parse(converted[1]).date())\n",
    "                adder = datetime.datetime.combine(adder.tolist(),datetime.datetime.min.time())\n",
    "                return np.append(dates,adder) # numpy range is not endpoint inclusive\n",
    "            else:\n",
    "                pass\n",
    "            return np.array(dates)\n",
    "\n",
    "# def gdeltRangeString(element,coverage = None):\n",
    "#     \"\"\"Takes a numpy datetime and converts to string\"\"\"\n",
    "    \n",
    "#     ########################\n",
    "#     # Numpy datetime to object\n",
    "#     ########################\n",
    "#     element = element.tolist()\n",
    "    \n",
    "#     #########################\n",
    "#     # Current day check\n",
    "#     #########################\n",
    "#     if element == datetime.datetime.now().date():\n",
    "#         hour = datetime.datetime.now().hour\n",
    "#         multiplier = datetime.datetime.now().minute / 15\n",
    "#         multiple = 15 * multiplier\n",
    "#         conditioner =  multiplier +1\n",
    "#         converted = datetime.datetime.now().replace(minute=multiple,second=0).strftime('%Y%m%d%H%M%S')\n",
    "        \n",
    "#         ####################\n",
    "#         # Check for full data\n",
    "#         ####################\n",
    "#         if coverage:\n",
    "#             converted = restOfDay = np.array(\n",
    "#         map(\n",
    "#             lambda x: np.datetime64(parse(str(element)+\" \"+ x)\n",
    "#                                    ).tolist().strftime(\n",
    "#                 '%Y%m%d%H%M%S'\n",
    "#             ),times[:hour*4+conditioner]))\n",
    "            \n",
    "        \n",
    "        \n",
    "#     else:\n",
    "        \n",
    "#         if isinstance(element,list)==True:\n",
    "#             converted = map(lambda x: x,element)\n",
    "#         else:\n",
    "#             converted = (datetime.datetime.combine(element,datetime.time.min) + \n",
    "#                 datetime.timedelta(\n",
    "#                                     minutes=45,hours=23\n",
    "#                                     )\n",
    "#                                    ).strftime('%Y%m%d%H%M%S')\n",
    "#         if coverage:\n",
    "            \n",
    "#             converted = restOfDay = np.array(\n",
    "#             map(\n",
    "#                 lambda x: np.datetime64(parse('2016 10 15 '+ x)\n",
    "#                                        ).tolist().strftime(\n",
    "#                     '%Y%m%d%H%M%S'\n",
    "#                 ),times[:]))\n",
    "            \n",
    "\n",
    "            \n",
    "#     return converted\n",
    "\n",
    "\n",
    "def vectorizer(function,dateArray):\n",
    "    helper = np.vectorize(function)\n",
    "    \n",
    "    final = helper(dateArray.tolist()).tolist()\n",
    "    \n",
    "    if isinstance(final,list):\n",
    "        \n",
    "        final = list(set(final))\n",
    "    elif isinstance(final,str):\n",
    "        final=final\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    return final\n",
    "\n",
    "# Finds the urls from an array of dates\n",
    "\n",
    "\n",
    "def UrlFinder(targetDate):\n",
    "    return masterdf[masterdf[2].str.contains(targetDate)]\n",
    "\n",
    "\n",
    "def vectorizedUrlFinder(function,urlList):\n",
    "    helper=np.vectorize(function)\n",
    "    return pd.concat(helper(urlList).tolist())\n",
    "\n",
    "\n",
    "def downloadVectorizer(function,urlList):\n",
    "    '''\n",
    "    test2 = downloadVectorizer(downloadAndExtract,b)\n",
    "    test2.columns=gkgHeaders.tableId.tolist()\n",
    "    '''\n",
    "    helper=np.vectorize(function)\n",
    "    return pd.concat(helper(urlList).tolist())\n",
    "\n",
    "\n",
    "date = \"2016 Oct 16\"\n",
    "date2 = \"2016 Mar 13\"\n",
    "date3 = [\"2016 Jan 1\",'2016 Oct 15']\n",
    "date4 = [\"2016 Jan 1\",'2016 Oct 15','2016 Apr 3']\n",
    "date5 = ['2016 Jan 20','2016 Apr 3', \"2015 Jun 8\", '2015 25 Dec']\n",
    "\n",
    "\n",
    "minutes =(map(str,range(00,60,15)))\n",
    "hours = (map(str,range(0,24)))\n",
    "times =[]\n",
    "for l in hours:\n",
    "    if int(l)<10:\n",
    "        l=\"0\"+l\n",
    "    for k in minutes:\n",
    "        if k == \"0\":\n",
    "            k='00'\n",
    "        times.append('{0}:{1}'.format(l,k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logic\n",
    "\n",
    "* Enter string date\n",
    "* parse date\n",
    "* check date (list or individual, valid or not)\n",
    "* check for 15 minute request or full day request\n",
    "* generate GDELT valid date string\n",
    "* generate GDELT valide url\n",
    "* download urls\n",
    "* add headers\n",
    "* concatenate into dataframe\n",
    "* output data in dataframe, csv, json, excel, stata,gbq, sql etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixed RangeString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "def gdeltRangeString(element,coverage = None):\n",
    "    \"\"\"Takes a numpy datetime and converts to string\"\"\"\n",
    "    \n",
    "    ########################\n",
    "    # Numpy datetime to object\n",
    "    ########################\n",
    "\n",
    "    \n",
    "    ########################\n",
    "#     Current day check\n",
    "    ########################\n",
    "    \n",
    "    element = element.tolist()\n",
    "    \n",
    "    hour = datetime.datetime.now().hour     \n",
    "    multiplier = datetime.datetime.now().minute / 15\n",
    "    multiple = 15 * multiplier\n",
    "    conditioner =  multiplier +1\n",
    "    \n",
    "    \n",
    "    \n",
    "    if isinstance(element,list)==False:\n",
    "        \n",
    "        \n",
    "        if element.date() == datetime.datetime.now().date():\n",
    "            if coverage:\n",
    "                print \"coverage current\"\n",
    "                converted = np.array(\n",
    "                        map(\n",
    "                            lambda x: np.datetime64(parse(str(element)+\" \"+ x)\n",
    "                            ).tolist().strftime(\n",
    "                                                '%Y%m%d%H%M%S'\n",
    "                                                ),times[:hour*4+conditioner]))\n",
    "            else:\n",
    "                converted = datetime.datetime.now().replace(minute=multiple,second=0).strftime('%Y%m%d%H%M%S')\n",
    "                \n",
    "        else:\n",
    "            if coverage:\n",
    "                \n",
    "                converted = restOfDay = np.array(\n",
    "                map(\n",
    "                    lambda x: np.datetime64(parse(str(element)+\" \"+ x)\n",
    "                                           ).tolist().strftime(\n",
    "                        '%Y%m%d%H%M%S'\n",
    "                    ),times[:]))\n",
    "            else:\n",
    "            \n",
    "\n",
    "                converted = element.replace(minute=multiple,second=0).strftime('%Y%m%d%H%M%S')\n",
    "\n",
    "            \n",
    "#         ####################\n",
    "#         # Check for full data\n",
    "#         ####################\n",
    "\n",
    "                \n",
    "#         if coverage:\n",
    "#             print \"where are you\"\n",
    "#             converted = np.array(\n",
    "#                     map(\n",
    "#                         lambda x: np.datetime64(parse(str(element)+\" \"+ x)\n",
    "#                         ).tolist().strftime(\n",
    "#                                             '%Y%m%d%H%M%S'\n",
    "#                                             ),times[:hour*4+conditioner]))\n",
    "#             else:\n",
    "                \n",
    "#                 converted = restOfDay = np.array(\n",
    "#                 map(\n",
    "#                     lambda x: np.datetime64(parse(str(element)+\" \"+ x)\n",
    "#                                            ).tolist().strftime(\n",
    "#                         '%Y%m%d%H%M%S'\n",
    "#                     ),times[:]))\n",
    "#         else:\n",
    "#             print \"going here\"\n",
    "#             hour = datetime.datetime.now().hour\n",
    "            \n",
    "#             multiplier = datetime.datetime.now().minute / 15\n",
    "#             multiple = 15 * multiplier\n",
    "#             conditioner =  multiplier +1\n",
    "#             converted = datetime.datetime.now().replace(minute=multiple,second=0).strftime('%Y%m%d%H%M%S')\n",
    "\n",
    "            \n",
    "            \n",
    "        \n",
    "    ####################\n",
    "    # All non-current dates\n",
    "    ####################    \n",
    "    \n",
    "    else:\n",
    "        \n",
    "        ####################\n",
    "        # Handling list\n",
    "        ####################  \n",
    "\n",
    "        if isinstance(element,list)==True:\n",
    "            \n",
    "#             converted = map(lambda x: x.strftime('%Y%m%d%H%M%S'),element)\n",
    "            converted = map(lambda x: (datetime.datetime.combine(x,datetime.time.min) + \n",
    "                datetime.timedelta(\n",
    "                                    minutes=45,hours=23\n",
    "                                    )\n",
    "                                   ).strftime('%Y%m%d%H%M%S'),element)\n",
    "        else:\n",
    "            print \"i'm here\"\n",
    "            converted = (datetime.datetime.combine(element,datetime.time.min) + \n",
    "                datetime.timedelta(\n",
    "                                    minutes=45,hours=23\n",
    "                                    )\n",
    "                                   ).strftime('%Y%m%d%H%M%S')\n",
    "        \n",
    "        ####################\n",
    "        # Return all 15 min intervals\n",
    "        #################### \n",
    "        if coverage:\n",
    "            \n",
    "            converted = []\n",
    "            for i in element:\n",
    " \n",
    "                converted.append(np.array(\n",
    "                map(\n",
    "                    lambda x: np.datetime64(parse(str(i)+\" \"+ x)\n",
    "                                           ).tolist().strftime(\n",
    "                        '%Y%m%d%H%M%S'\n",
    "                    ),times[:])))\n",
    "            converted = np.concatenate(converted,axis=0)\n",
    "            if len(converted.tolist())>=(3*192):\n",
    "                warnings.warn('\\n\\nThis query will download {0} files, and likely exhaust your memory with possibly 10s of GBs of data in this single query.  Hit Ctr-C to kill this query if you do not want to continue.'.format(len(converted.tolist())))\n",
    "            \n",
    "\n",
    "            \n",
    "    return converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coverage current\n"
     ]
    }
   ],
   "source": [
    "b =gdeltRangeString(dateRanger(date),coverage=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['20161016000000', '20161016001500', '20161016003000',\n",
       "       '20161016004500', '20161016010000', '20161016011500',\n",
       "       '20161016013000', '20161016014500', '20161016020000',\n",
       "       '20161016021500', '20161016023000', '20161016024500',\n",
       "       '20161016030000', '20161016031500', '20161016033000',\n",
       "       '20161016034500'], \n",
       "      dtype='|S14')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixing the Dates so we don't need to download master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'datetime.datetime' object has no attribute 'tolist'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-9af762b762da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m pd.DataFrame(gdeltRangeString(dateRanger(date3)[-1],coverage=True))[0].apply(\n\u001b[0m\u001b[1;32m      2\u001b[0m lambda x: \"http://data.gdeltproject.org/gdeltv2/\"+x+\".export.CSV.zip\")\n",
      "\u001b[0;32m<ipython-input-4-f24eebe1d2a3>\u001b[0m in \u001b[0;36mgdeltRangeString\u001b[0;34m(element, coverage)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m########################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0melement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mhour\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhour\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'datetime.datetime' object has no attribute 'tolist'"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(gdeltRangeString(dateRanger(date3)[-1],coverage=True))[0].apply(\n",
    "lambda x: \"http://data.gdeltproject.org/gdeltv2/\"+x+\".export.CSV.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def urlBuilder(dateString,table='events'):\n",
    "    '''\n",
    "    Takes date string from gdeltRange string and creates GDELT urls\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ------------\n",
    "    \n",
    "    table types:\n",
    "                * events and mentions (default)\n",
    "                * gkg\n",
    "                * mentions only\n",
    "    '''\n",
    "    \n",
    "    if table == 'events' or table == '':\n",
    "        eventsUrl = \"http://data.gdeltproject.org/gdeltv2/\"+dateString+\".export.CSV.zip\"\n",
    "        \n",
    "        return eventsUrl\n",
    "    elif table == 'mentionsOnly':\n",
    "        mentionsUrl = \"http://data.gdeltproject.org/gdeltv2/\"+dateString+\".mentions.CSV.zip\"\n",
    "        return mentionsUrl\n",
    "    elif table == 'gkg':\n",
    "        return \"http://data.gdeltproject.org/gdeltv2/\"+dateString+\".gkg.csv.zip\"\n",
    "    else:\n",
    "        print \"You have entered a wrong table\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "urls = []\n",
    "for l in b:\n",
    "    urls.append(urlBuilder(l,'events'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2429, 61)\n",
      "(2802, 61)\n",
      "(2279, 61)\n",
      "(1680, 61)\n",
      "(1577, 61)\n",
      "(1874, 61)\n",
      "(1582, 61)\n",
      "(1794, 61)\n",
      "(1643, 61)\n",
      "(1426, 61)\n",
      "(1892, 61)\n",
      "(1482, 61)\n",
      "(1151, 61)\n",
      "(1276, 61)\n",
      "(1087, 61)\n",
      "(1210, 61)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "import requests \n",
    "\n",
    "dfs = []\n",
    "for l in urls:\n",
    "    r = requests.get(l)\n",
    "    try:\n",
    "        frame = pd.read_csv(BytesIO(r.content),compression='zip',sep = '\\t',header=None)\n",
    "        dfs.append(frame)\n",
    "        print frame.shape\n",
    "        del frame\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81, 100, 121, 144, 169, 196, 225, 256, 289, 324, 361, 400, 441, 484, 529, 576, 625, 676, 729, 784, 841, 900, 961, 1024, 1089, 1156, 1225, 1296, 1369, 1444, 1521, 1600, 1681, 1764, 1849, 1936, 2025, 2116, 2209, 2304, 2401]\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "\n",
    "def g():\n",
    "    for el in xrange(50):\n",
    "        print el\n",
    "        yield el\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    time.sleep(1)\n",
    "    return x * x\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pool = mp.Pool(processes=4)              # start 4 worker processes\n",
    "    go = g()\n",
    "    result = []\n",
    "    N = 11\n",
    "    while True:\n",
    "        g2 = pool.map(f, itertools.islice(go, N))\n",
    "        if g2:\n",
    "            result.extend(g2)\n",
    "            time.sleep(1)\n",
    "        else:\n",
    "            break\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 4,\n",
       " 9,\n",
       " 16,\n",
       " 25,\n",
       " 36,\n",
       " 49,\n",
       " 64,\n",
       " 81,\n",
       " 100,\n",
       " 121,\n",
       " 144,\n",
       " 169,\n",
       " 196,\n",
       " 225,\n",
       " 256,\n",
       " 289,\n",
       " 324,\n",
       " 361,\n",
       " 400,\n",
       " 441,\n",
       " 484,\n",
       " 529,\n",
       " 576,\n",
       " 625,\n",
       " 676,\n",
       " 729,\n",
       " 784,\n",
       " 841,\n",
       " 900,\n",
       " 961,\n",
       " 1024,\n",
       " 1089,\n",
       " 1156,\n",
       " 1225,\n",
       " 1296,\n",
       " 1369,\n",
       " 1444,\n",
       " 1521,\n",
       " 1600,\n",
       " 1681,\n",
       " 1764,\n",
       " 1849,\n",
       " 1936,\n",
       " 2025,\n",
       " 2116,\n",
       " 2209,\n",
       " 2304,\n",
       " 2401]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(urls[0], compression='zip',sep='\\t',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from blaze import data\n",
    "import dask.dataframe as dd\n",
    "import dask.bag as db\n",
    "from dask import delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def reader(filename):\n",
    "    return pd.read_csv(filename,compression='zip',sep='\\t',header=None)\n",
    "\n",
    "# build list of delayed pandas csv reads; then read in as dask dataframe\n",
    "\n",
    "# dfs = [delayed(reader)(fn) for fn in urls]\n",
    "# df = dd.from_delayed(dfs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "urls[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = requests.get('http://data.gdeltproject.org/gdeltv2/20160313044500.export.CSV.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "\n",
    "df = pd.read_csv(BytesIO(r.content),compression='zip',sep='\\t',header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'resulst' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-e0ed809bcfb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresulst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'resulst' is not defined"
     ]
    }
   ],
   "source": [
    "for l in urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "pool = mp.Pool()\n",
    "results = list(pool.imap_unordered(mp_worker, urls))\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Download Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "import time\n",
    "import zipfile\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "\n",
    "def mp_worker(url):\n",
    "    start = datetime.datetime.now()\n",
    "    r = requests.get(url)\n",
    "    frame = pd.read_csv(BytesIO(r.content),compression='zip',sep='\\t',header=None)\n",
    "    end = datetime.datetime.now() - start\n",
    "    return frame\n",
    "    \n",
    "    \n",
    "\n",
    "# def mp_handler():\n",
    "#     result = []\n",
    "#     p = multiprocessing.Pool(4)\n",
    "#     dfs = p.map(mp_worker, urls)\n",
    "#     result.extend(dfs)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    result = []\n",
    "    p = multiprocessing.Pool(4)\n",
    "    dfs = p.imap_unordered(mp_worker,urls)\n",
    "    if dfs:\n",
    "        result.extend(dfs)\n",
    "        time.sleep(1)\n",
    "    else:\n",
    "        pass\n",
    "    print(len(result))\n",
    "    \n",
    "# http://stackoverflow.com/questions/5318936/python-multiprocessing-pool-lazy-iteration    \n",
    "# http://stackoverflow.com/questions/20577472/how-to-keep-track-of-asynchronous-results-returned-from-a-multiprocessing-pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.concat(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 27184 entries, 0 to 1209\n",
      "Data columns (total 61 columns):\n",
      "0     27184 non-null int64\n",
      "1     27184 non-null int64\n",
      "2     27184 non-null int64\n",
      "3     27184 non-null int64\n",
      "4     27184 non-null float64\n",
      "5     24326 non-null object\n",
      "6     24326 non-null object\n",
      "7     15734 non-null object\n",
      "8     212 non-null object\n",
      "9     194 non-null object\n",
      "10    373 non-null object\n",
      "11    78 non-null object\n",
      "12    10944 non-null object\n",
      "13    674 non-null object\n",
      "14    23 non-null object\n",
      "15    19117 non-null object\n",
      "16    19117 non-null object\n",
      "17    12306 non-null object\n",
      "18    233 non-null object\n",
      "19    134 non-null object\n",
      "20    364 non-null object\n",
      "21    72 non-null object\n",
      "22    8571 non-null object\n",
      "23    500 non-null object\n",
      "24    7 non-null object\n",
      "25    27184 non-null int64\n",
      "26    27184 non-null int64\n",
      "27    27184 non-null int64\n",
      "28    27184 non-null int64\n",
      "29    27184 non-null int64\n",
      "30    27184 non-null float64\n",
      "31    27184 non-null int64\n",
      "32    27184 non-null int64\n",
      "33    27184 non-null int64\n",
      "34    27184 non-null float64\n",
      "35    27184 non-null int64\n",
      "36    23583 non-null object\n",
      "37    23594 non-null object\n",
      "38    23594 non-null object\n",
      "39    13982 non-null object\n",
      "40    23583 non-null float64\n",
      "41    23585 non-null float64\n",
      "42    23594 non-null object\n",
      "43    27184 non-null int64\n",
      "44    18626 non-null object\n",
      "45    18635 non-null object\n",
      "46    18635 non-null object\n",
      "47    9849 non-null object\n",
      "48    18626 non-null float64\n",
      "49    18627 non-null float64\n",
      "50    18635 non-null object\n",
      "51    27184 non-null int64\n",
      "52    26331 non-null object\n",
      "53    26349 non-null object\n",
      "54    26349 non-null object\n",
      "55    13909 non-null object\n",
      "56    26331 non-null float64\n",
      "57    26341 non-null float64\n",
      "58    26349 non-null object\n",
      "59    27184 non-null int64\n",
      "60    27184 non-null object\n",
      "dtypes: float64(9), int64(16), object(36)\n",
      "memory usage: 12.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "def worker():\n",
    "    r.con\n",
    "    \"\"\"worker function\"\"\"\n",
    "    print 'Worker'\n",
    "    return\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    jobs = []\n",
    "    for i in range(5):\n",
    "        p = multiprocessing.Process(target=worker)\n",
    "        jobs.append(p)\n",
    "        p.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dfs = easy_parallize(mp_worker,urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from multiprocessing.pool import Pool\n",
    "#from multiprocessing.pool import ThreadPool as Pool  # to use threads\n",
    "\n",
    "with multiprocessing.Pool() as pool:\n",
    "    answers = []\n",
    "    for i in urls:\n",
    "        result1 = pool.apply_async(mp_worker, i)\n",
    "        answers.append(result1.get())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with multiprocessing.pool.Pool as pool:\n",
    "    print l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ddf = dd.read_csv('/Users/linwood/Downloads/20150218230000.export.CSV.zip',sep='\\t',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "table = ''\n",
    "\n",
    "if table == 'events' or table == '' or table == 'mentions':\n",
    "    print \"Yes\"\n",
    "elif table == 'gkg':\n",
    "    print \"gkg\"\n",
    "else:\n",
    "    print \"You have entered a wrong table\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hour = datetime.datetime.now().hour\n",
    "multiplier = datetime.datetime.now().minute / 15\n",
    "multiple = 15 * multiplier\n",
    "conditioner = multiplier +1\n",
    "np.array(map(lambda x: np.datetime64(parse(str(dateRanger(date3)[-1])+\" \" + x)).tolist().strftime('%Y%m%d%H%M%S'),times[:hour*4+conditioner])).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gdeltRangeString(dateRanger(['2016 Oct 1 10:32']).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "minutes =(map(str,range(00,60,15)))\n",
    "hours = (map(str,range(0,24)))\n",
    "times =[]\n",
    "for l in hours:\n",
    "    if int(l)<10:\n",
    "        l=\"0\"+l\n",
    "    for k in minutes:\n",
    "        if k == \"0\":\n",
    "            k='00'\n",
    "        times.append('{0}:{1}'.format(l,k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dateRanger(['2016 Oct 15']).tolist().date() == datetime.datetime.today().date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.datetime64(parse('2016 10 11 '+ times[17]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer(gdeltRangeString,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.datetime64(parse('2016 10 14'),dtype='M8[h]').tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working Examples for Single Date Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "date = '2016 9 12'\n",
    "\n",
    "gdeltRangeString(np.datetime64(parse('2016 10 14')).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working Examples of Date Range Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "date=['2016 09 01','2016 09 24']\n",
    "(dateRanger(date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# converts to gd\n",
    "datesToPull = vectorizer(gdeltRangeString,dateRanger(['2016 Oct 13','2016 Oct 15']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datesToPull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "masterdf[masterdf[2].str.contains(datesToPull)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# gets the urls from array\n",
    "resultMaster = vectorizedUrlFinder(UrlFinder,datesToPull)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Area for Dates; Above is good, below is experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tblCheck('gkg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test2.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datesToPull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****************\n",
    "\n",
    "## Munging Data: Extracting Specific Datasets or all of them\n",
    "\n",
    "Work with the returned GDELT data.  Specific whether we are pulling the `mentions`, `events`, or `gkg` date for the day or all.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = match_date(gdelt_timeString(dateInputCheck(date)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target = results[2][results[2].str.contains('export')].reset_index(drop=True).ix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# GDELT data download and extraction\n",
    "#############################################\n",
    "\n",
    "from StringIO import StringIO\n",
    "import pandas as pd\n",
    "import requests\n",
    "import zipfile\n",
    "import re\n",
    "\n",
    "def downloadAndExtract(gdeltUrl):\n",
    "    \"\"\"Downloads and extracts GDELT zips without saving to disk\"\"\"\n",
    "    \n",
    "    response = requests.get(gdeltUrl, stream=True)\n",
    "    zipdata = StringIO()\n",
    "    zipdata.write(response.content)\n",
    "    gdelt_zipfile = zipfile.ZipFile(zipdata,'r')\n",
    "    name = re.search('(([\\d]{4,}).*)',gdelt_zipfile.namelist()[0]).group().replace('.zip',\"\")\n",
    "    data = gdelt_zipfile.read(name)\n",
    "    gdelt_zipfile.close()\n",
    "    del zipdata,gdelt_zipfile,name,response\n",
    "    return pd.read_csv(StringIO(data),delimiter='\\t',header=None)\n",
    "    \n",
    "\n",
    "def add_header(gdeltUrl):\n",
    "    \"\"\"Returns the header rows for the dataframe\"\"\"\n",
    "    \n",
    "    dbType = re.search(\n",
    "        '(mentions|export|gkg)',\n",
    "        gdeltUrl\n",
    "        ).group()\n",
    "    \n",
    "    if dbType == \"gkg\":\n",
    "        headers = gkgHeaders.tableId.tolist()\n",
    "    \n",
    "    elif dbType == \"mentions\":\n",
    "        headers = mentionsHeaders.tableId.tolist()\n",
    "        \n",
    "    elif dbType == \"export\":\n",
    "        headers = eventsDbHeaders.tableId.tolist()\n",
    "        \n",
    "    return headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target = 'http://data.gdeltproject.org/gdeltv2/20160924150000.export.CSV.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gdelt_df = downloadAndExtract(target)\n",
    "gdelt_df.columns = add_header(target)\n",
    "gdelt_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combined = gdelt_df.merge(gdelt_df2,how='outer',on='GLOBALEVENTID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combined.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combined.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# combined.[(combined.Confidence != None) & (combined.MonthYear != None)]\n",
    "combined[['Actor1Code','Actor1Name']][(combined.GoldsteinScale <= -5.2) & (combined.Actor1Code != \"\")].fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Early Pipeline to Write out R Dataframe\n",
    "\n",
    "\n",
    "Ways to install\n",
    "```python\n",
    "pip install feather-format\n",
    "```\n",
    "\n",
    "```bash\n",
    "conda install feather-format -c conda-forge\n",
    "```\n",
    "\n",
    "\n",
    "###  **IT WORKS!!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import feather\n",
    "path = 'my_data.feather'\n",
    "feather.api.write_dataframe(testdf, path)\n",
    "newtestdf = feather.api.read_dataframe(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leftovers; Junkyard below (stuff to work on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = masterListdf[masterListdf[2].str.contains(gdelt_timeString(dateInputCheck(date)))==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results[2].reset_index().ix[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results[results[2].str.contains('gkg')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gdelt_timeString(dateInputCheck(date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from dateutil.parser import parse\n",
    "re.search('(([\\d]{4,}).*)',clean[20][-1]).group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if bool(4>3):\n",
    "    print \"Hello\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(datetime.datetime.now().replace(hour=0,minute=0,second=0,microsecond=0)) == parse(\"2016 09 18\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b = dateutil.parser.parse(re.search('([\\d]{4,})',clean[20][-1]).group())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matchDate = re.search('([\\d]{4,})',clean[20][-1]).group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def time_change(current,diff):\n",
    "    date = current.replace(minute=0, second=0) + timedelta(minutes=diff)\n",
    "    return date.strftime(\"%Y%m%d%H%M%S\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pulling most current daily report\n",
    "\n",
    "import numpy as np\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "currentTime = datetime.datetime.now()\n",
    "timeDiff = currentTime.minute / 15 \n",
    "\n",
    "query = np.where(timeDiff == 1,time_change(currentTime,diff=15),\n",
    "        np.where(timeDiff == 2, time_change(currentTime,diff=30),\n",
    "                 np.where(timeDiff == 3, time_change(currentTime,diff=45),\n",
    "                          time_change(currentTime,diff=0))))\n",
    "\n",
    "baseUrl = 'http://data.gdeltproject.org/gdeltv2/' + str(query) + '.export.CSV.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "myzipfile.namelist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import zipfile\n",
    "\n",
    "\n",
    "r = requests.get(baseUrl, stream=True)\n",
    "\n",
    "# with open('gdelt.zip', 'wb') as f:\n",
    "#     f.write(r.content)\n",
    "# fh = open('gdelt.zip')\n",
    "# g = zipfile.ZipFile(fh)\n",
    "# g.extractall()\n",
    "\n",
    "from StringIO import StringIO\n",
    "zipdata = StringIO()\n",
    "zipdata.write(r.content)\n",
    "myzipfile = zipfile.ZipFile(zipdata,'r')\n",
    "data = myzipfile.read(str(query) + '.export.CSV')\n",
    "gdeltdf = pd.read_csv(StringIO(data),delimiter='\\t',header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gdeltdf.columns=headers.tableId.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gdeltdf.SOURCEURL[((gdeltdf.ActionGeo_CountryCode =='SY')|(gdeltdf.ActionGeo_CountryCode =='IZ')) & (gdeltdf.GoldsteinScale < -4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = '''\n",
    "GLOBALEVENTID\tINTEGER\tNULLABLE\tThis is the ID of the event that was mentioned in the article.\n",
    "EventTimeDate\tINTEGER\tNULLABLE\tThis is the 15-minute timestamp (YYYYMMDDHHMMSS) when the event being mentioned was first recorded by GDELT (the DATEADDED field of the original event record).  This field can be compared against the next one to identify events being mentioned for the first time (their first mentions) or to identify events of a particular vintage being mentioned now (such as filtering for mentions of events at least one week old).\n",
    "MentionTimeDate\tINTEGER\tNULLABLE\tThis is the 15-minute timestamp (YYYYMMDDHHMMSS) of the current update.  This is identical for all entries in the update file but is included to make it easier to load the Mentions table into a database.\n",
    "MentionType\tINTEGER\tNULLABLE\tThis is a numeric identifier that refers to the source collection the document came from and is used to interpret the MentionIdentifier in the next column.  In essence, it specifies how to interpret the MentionIdentifier to locate the actual document.  At present, it can hold one of the following values:o 1 = WEB (The document originates from the open web and the MentionIdentifier is a fully-qualified URL that can be used to access the document on the web).o 2 = CITATIONONLY (The document originates from a broadcast, print, or other offline source in which only a textual citation is available for the document.  In this case the MentionIdentifier contains the textual citation for the document).o 3 = CORE (The document originates from the CORE archive and the MentionIdentifier contains its DOI, suitable for accessing the original document through the CORE website).o 4 = DTIC (The document originates from the DTIC archive and the MentionIdentifier contains its DOI, suitable for accessing the original document through the DTIC website).o 5 = JSTOR (The document originates from the JSTOR archive and the MentionIdentifier contains its DOI, suitable for accessing the original document through your JSTOR subscription if your institution subscribes to it).o 6 = NONTEXTUALSOURCE (The document originates from a textual proxy (such as closed captioning) of a non-textual information source (such as a video) available via a URL and the MentionIdentifier provides the URL of the non-textual original source.  At present, this Collection Identifier is used for processing of the closed captioning streams of the Internet Archive Television News Archive in which each broadcast is available via a URL, but the URL offers access only to the video of the broadcast and does not provide any access to the textual closed captioning used to generate the metadata.  This code is used in order to draw a distinction between URL-based textual material (Collection Identifier 1 (WEB) and URL-based non-textual material like the Television News Archive).\n",
    "MentionSourceName\tSTRING\tNULLABLE\tThis is a human-friendly identifier of the source of the document.  For material originating from the open web with a URL this field will contain the top-level domain the page was from.  For BBC Monitoring material it will contain BBC Monitoring and for JSTOR material it will contain JSTOR.  This field is intended for human display of major sources as well as for network analysis of information flows by source, obviating the requirement to perform domain or other parsing of the MentionIdentifier field.\n",
    "MentionIdentifier\tSTRING\tNULLABLE\tThis is the unique external identifier for the source document.  It can be used to uniquely identify the document and access it if you have the necessary subscriptions or authorizations and/or the document is public access.  This field can contain a range of values, from URLs of open web resources to textual citations of print or broadcast material to DOI identifiers for various document repositories.  For example, if MentionType is equal to 1, this field will contain a fully-qualified URL suitable for direct access.  If MentionType is equal to 2, this field will contain a textual citation akin to what would appear in an academic journal article referencing that document (NOTE that the actual citation format will vary (usually between APA, Chicago, Harvard, or MLA) depending on a number of factors and no assumptions should be made on its precise format at this time due to the way in which this data is currently provided to GDELT  future efforts will focus on normalization of this field to a standard citation format).  If MentionType is 3, the field will contain a numeric or alpha-numeric DOI that can be typed into JSTORs search engine to access the document if your institution has a JSTOR subscription.\n",
    "SentenceID\tINTEGER\tNULLABLE\tThe sentence within the article where the event was mentioned (starting with the first sentence as 1, the second sentence as 2, the third sentence as 3, and so on).  This can be used similarly to the CharOffset fields below, but reports the events location in the article in terms of sentences instead of characters, which is more amenable to certain measures of the importance of an events positioning within an article.\n",
    "Actor1CharOffset\tINTEGER\tNULLABLE\tThe location within the article (in terms of English characters) where Actor1 was found.  This can be used in combination with the GKG or other analysis to identify further characteristics and attributes of the actor.  NOTE: due to processing performed on each article, this may be slightly offset from the position seen when the article is rendered in a web browser.\n",
    "Actor2CharOffset\tINTEGER\tNULLABLE\tThe location within the article (in terms of English characters) where Actor2 was found.  This can be used in combination with the GKG or other analysis to identify further characteristics and attributes of the actor.  NOTE: due to processing performed on each article, this may be slightly offset from the position seen when the article is rendered in a web browser.\n",
    "ActionCharOffset\tINTEGER\tNULLABLE\tThe location within the article (in terms of English characters) where the core Action description was found.  This can be used in combination with the GKG or other analysis to identify further characteristics and attributes of the actor.  NOTE: due to processing performed on each article, this may be slightly offset from the position seen when the article is rendered in a web browser.\n",
    "InRawText\tINTEGER\tNULLABLE\tThis records whether the event was found in the original unaltered raw article text (a value of 1) or whether advanced natural language processing algorithms were required to synthesize and rewrite the article text to identify the event (a value of 0).  See the discussion on the Confidence field below for more details.  Mentions with a value of 1 in this field likely represent strong detail-rich references to an event.\n",
    "Confidence\tINTEGER\tNULLABLE\tPercent confidence in the extraction of this event from this article.  See the discussion in the codebook at http://data.gdeltproject.org/documentation/GDELT-Event_Codebook-V2.0.pdf\n",
    "MentionDocLen\tINTEGER\tNULLABLE\tThe length in English characters of the source document (making it possible to filter for short articles focusing on a particular event versus long summary articles that casually mention an event in passing).\n",
    "MentionDocTone\tFLOAT\tNULLABLE\tThe same contents as the AvgTone field in the Events table, but computed for this particular article.  NOTE: users interested in emotional measures should use the MentionIdentifier field above to merge the Mentions table with the GKG table to access the complete set of 2,300 emotions and themes from the GCAM system.\n",
    "MentionDocTranslationInfo\tSTRING\tNULLABLE\tThis field is internally delimited by semicolons and is used to record provenance information for machine translated documents indicating the original source language and the citation of the translation system used to translate the document for processing.  It will be blank for documents originally in English.  At this time the field will also be blank for documents translated by a human translator and provided to GDELT in English (such as BBC Monitoring materials)  in future this field may be expanded to include information on human translation pipelines, but at present it only captures information on machine translated materials.  An example of the contents of this field might be srclc:fra; eng:Moses 2.1.1 / MosesCore Europarl fr-en / GT-FRA 1.0.  NOTE:  Machine translation is often not as accurate as human translation and users requiring the highest possible confidence levels may wish to exclude events whose only mentions are in translated reports, while those needing the highest-possible coverage of the non-Western world will find that these events often offer the earliest glimmers of breaking events or smaller-bore events of less interest to Western media.o SRCLC. This is the Source Language Code, representing the three-letter ISO639-2 code of the language of the original source material. o ENG.  This is a textual citation string that indicates the engine(s) and model(s) used to translate the text.  The format of this field will vary across engines and over time and no expectations should be made on the ordering or formatting of this field.  In the example above, the string Moses 2.1.1 / MosesCore Europarl fr-en / GT-FRA 1.0 indicates that the document was translated using version 2.1.1 of the Moses   SMT platform, using the MosesCore Europarl fr-en translation and language models, with the final translation enhanced via GDELT Translinguals own version 1.0 French translation and language models.  A value of GT-ARA 1.0 indicates that GDELT Translinguals version 1.0 Arabic translation and language models were the sole resources used for translation.  Additional language systems used in the translation pipeline such as word segmentation systems are also captured in this field such that a value of GT-ZHO 1.0 / Stanford PKU indicates that the Stanford Chinese Word Segmenter   was used to segment the text into individual words and sentences, which were then translated by GDELT Translinguals own version 1.0 Chinese (Traditional or Simplified) translation and language models.\n",
    "Extras\tSTRING\tNULLABLE\tThis field is currently blank, but is reserved for future use to encode special additional measurements for selected material.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from StringIO import StringIO\n",
    "eventMentions = pd.read_csv(StringIO(text),delimiter='\\t',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eventMentions.columns=['tableId', 'dataType','Empty', 'Description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eventMentions.to_csv('../../gdelt2HeaderRows/schema_csvs/GDELT_2.0_eventMentions_Column_Labels_Header_Row_Sep2016.tsv',encoding='utf-8',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eventMentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gkgdf.to_csv('../../gdelt2HeaderRows/schema_csvs/GDELT_2.0_gdeltKnowledgeGraph_Column_Labels_Header_Row_Sep2016.tsv',encoding='utf-8',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gkgdf.to_csv('GDELT_2.0_gdeltKnowledgeGraph_Column_Labels_Header_Row_Sep2016.csv',sep='\\t',index=False,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "headers.to_csv('GDELT_2.0_Events_Column_Labels_Header_Row_Sep2016.csv', index=False,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "mentionsdf = pd.read_csv(StringIO(text),delimiter='\\t',header=None)\n",
    "mentionsdf.columns=headers.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*********************\n",
    "# Building the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = 30\n",
    "x1 = np.linspace(-2, 2, N)\n",
    "x1[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
